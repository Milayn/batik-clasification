{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.9.4)\n",
      "Requirement already satisfied: absl-py in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.7.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (4.66.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow_datasets) (0.5.1)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/python/3.10.13/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in /home/codespace/.local/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.10.0)\n",
      "Requirement already satisfied: zipp in /usr/local/python/3.10.13/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.2.2)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.63.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow_datasets\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 16:56:50.605301: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 16:56:51.606676: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 16:56:53.918940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 16:56:57.150112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 960 files belonging to 60 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 16:58:49.322819: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1\n",
      "Train Index [  0   1   3   4   5   6   8   9  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  24  25  26  27  28  29  32  34  35  36  37  38  40  41  42  43\n",
      "  44  45  46  47  48  50  51  52  53  56  57  58  59  60  61  62  64  68\n",
      "  69  70  71  73  74  75  80  82  83  85  87  88  89  90  91  92  93  94\n",
      "  95  96  98  99 100 102 103 104 105 106 107 108 110 111 112 113 114 115\n",
      " 116 117 119 121 122 123 124 125 126 127 128 129 130 131 132 134 135 136\n",
      " 138 139 140 141 142 143 144 145 146 147 149 150 151 152 153 154 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 169 170 171 172 173 175 176 177\n",
      " 178 179 180 182 183 184 185 186 187 188 189 190 191 194 195 197 198 200\n",
      " 201 202 203 205 206 207 214 216 217 219 221 222 223 224 225 226 228 229\n",
      " 230 231 232 233 235 236 237 238 240 241 242 243 245 246 247 248 249 251\n",
      " 252 253 255 256 257 258 260 261 262 263 264 266 267 268 269 270 271 272\n",
      " 273 274 276 277 278 279 280 282 283 284 285 286 287 288 289 291 292 293\n",
      " 294 295 297 298 301 302 303 304 305 306 307 308 310 311 312 313 315 316\n",
      " 317 318 319 320 321 322 323 325 326 327 329 330 332 333 334 335 337 339\n",
      " 340 341 342 343 345 346 347 348 349 351 352 353 354 357 358 359 360 364\n",
      " 365 366 367 368 369 370 371 372 373 374 376 377 378 379 380 381 384 385\n",
      " 386 387 388 389 390 391 392 393 395 397 398 399 400 401 402 403 406 407\n",
      " 408 410 412 413 414 415 416 417 418 419 420 421 422 423 424 426 427 429\n",
      " 430 432 434 435 436 437 438 439 440 441 442 443 444 445 446 447 449 450\n",
      " 452 453 454 455 458 459 460 461 463 464 465 466 467 469 471 472 474 475\n",
      " 476 477 478 479 480 481 484 485 488 489 491 492 493 495 496 497 498 499\n",
      " 500 502 503 504 505 506 507 508 509 510 514 515 517 518 520 521 522 523\n",
      " 524 525 526 527 529 530 531 532 533 535 536 537 538 539 540 541 542 543\n",
      " 545 546 547 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564\n",
      " 565 566 567 569 570 571 572 573 574 575 576 577 578 580 583 584 585 586\n",
      " 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 607 608 609\n",
      " 610 611 612 613 614 615 616 618 619 621 622 623 625 626 627 628 630 631\n",
      " 632 633 634 637 638 640 641 642 643 644 645 646 647 649 650 651 652 653\n",
      " 654 655 656 657 658 660 661 663 664 665 666 669 670 671 672 673 674 675\n",
      " 676 678 679 680 681 682 683 684 685 686 687 688 689 693 695 696 697 698\n",
      " 699 700 701 702 703 704 705 707 708 709 710 711 712 713 715 717 718 719\n",
      " 721 722 723 724 726 727 729 730 732 733 735 736 737 738 739 740 741 742\n",
      " 743 744 745 746 747 749 750 751 752 753 755 756 757 758 759 761 762 765\n",
      " 766 767]\n",
      "Test Index [  2   7  10  23  30  31  33  39  49  54  55  63  65  66  67  72  76  77\n",
      "  78  79  81  84  86  97 101 109 118 120 133 137 148 155 168 174 181 192\n",
      " 193 196 199 204 208 209 210 211 212 213 215 218 220 227 234 239 244 250\n",
      " 254 259 265 275 281 290 296 299 300 309 314 324 328 331 336 338 344 350\n",
      " 355 356 361 362 363 375 382 383 394 396 404 405 409 411 425 428 431 433\n",
      " 448 451 456 457 462 468 470 473 482 483 486 487 490 494 501 511 512 513\n",
      " 516 519 528 534 544 548 549 568 579 581 582 587 603 604 605 606 617 620\n",
      " 624 629 635 636 639 648 659 662 667 668 677 690 691 692 694 706 714 716\n",
      " 720 725 728 731 734 748 754 760 763 764]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.0440 - loss: 0.3554 - val_accuracy: 0.2857 - val_loss: 0.1162\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.6446 - loss: 0.0563 - val_accuracy: 0.8182 - val_loss: 0.0307\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.9196 - loss: 0.0168 - val_accuracy: 0.9351 - val_loss: 0.0153\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.9933 - loss: 0.0068 - val_accuracy: 0.9675 - val_loss: 0.0093\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9870 - val_loss: 0.0077\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9805 - val_loss: 0.0076\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9805 - val_loss: 0.0079\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9870 - val_loss: 0.0055\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9805 - val_loss: 0.0064\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 9.2876e-04 - val_accuracy: 0.9870 - val_loss: 0.0061\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step\n",
      "Predicted Classes: [40 28  3 14 58 20 55 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      "  4 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 14 54 37 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 2\n",
      "Train Index [  0   1   2   3   4   5   7   8   9  10  12  13  14  16  17  19  20  21\n",
      "  22  23  25  26  27  30  31  32  33  34  35  36  37  38  39  40  45  46\n",
      "  47  48  49  50  52  53  54  55  57  58  59  62  63  64  65  66  67  68\n",
      "  71  72  75  76  77  78  79  80  81  84  85  86  87  88  91  93  94  95\n",
      "  96  97  98  99 100 101 102 103 105 106 107 109 111 112 113 115 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 130 133 134 137 138 141\n",
      " 142 143 144 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161\n",
      " 162 166 168 169 170 171 172 173 174 175 179 180 181 183 184 185 186 187\n",
      " 188 189 190 191 192 193 194 195 196 197 199 200 201 202 203 204 205 206\n",
      " 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 224 225\n",
      " 226 227 228 229 230 232 233 234 236 237 238 239 240 241 242 243 244 245\n",
      " 246 249 250 251 252 253 254 255 258 259 261 262 263 265 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 287 288 289\n",
      " 290 293 295 296 297 298 299 300 301 303 304 305 307 308 309 310 312 313\n",
      " 314 315 316 317 318 320 321 322 324 325 328 330 331 332 336 337 338 339\n",
      " 341 343 344 345 347 348 349 350 353 354 355 356 358 359 361 362 363 364\n",
      " 365 366 368 369 370 371 372 373 374 375 376 378 379 382 383 384 385 386\n",
      " 387 389 390 391 392 394 396 397 398 399 400 401 402 403 404 405 406 407\n",
      " 408 409 410 411 413 414 415 416 418 419 420 421 422 423 424 425 426 427\n",
      " 428 430 431 433 434 435 436 437 438 441 442 444 445 447 448 450 451 452\n",
      " 454 455 456 457 458 459 460 461 462 463 466 467 468 469 470 471 472 473\n",
      " 474 475 476 479 480 482 483 484 486 487 488 489 490 491 492 493 494 495\n",
      " 496 498 499 500 501 502 503 504 505 507 508 509 510 511 512 513 515 516\n",
      " 517 519 520 521 522 523 524 526 527 528 530 531 533 534 536 537 540 541\n",
      " 542 543 544 545 546 548 549 550 551 552 553 555 556 558 560 561 562 563\n",
      " 564 565 566 567 568 569 571 573 574 575 576 577 578 579 580 581 582 584\n",
      " 585 587 588 590 591 592 595 596 597 598 600 601 602 603 604 605 606 607\n",
      " 609 610 611 612 613 614 617 620 621 622 623 624 625 627 628 629 630 633\n",
      " 634 635 636 638 639 641 642 644 645 646 647 648 649 650 651 652 653 654\n",
      " 655 656 657 658 659 660 661 662 663 664 665 667 668 669 670 672 674 676\n",
      " 677 678 680 681 683 684 686 687 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 701 702 704 706 708 709 710 711 712 714 715 716 717 720 721\n",
      " 722 723 724 725 726 727 728 729 730 731 732 733 734 736 737 738 739 740\n",
      " 741 742 743 744 747 748 750 751 752 753 754 756 757 759 760 761 763 764\n",
      " 765 767]\n",
      "Test Index [  6  11  15  18  24  28  29  41  42  43  44  51  56  60  61  69  70  73\n",
      "  74  82  83  89  90  92 104 108 110 114 131 132 135 136 139 140 145 158\n",
      " 163 164 165 167 176 177 178 182 198 223 231 235 247 248 256 257 260 264\n",
      " 266 286 291 292 294 302 306 311 319 323 326 327 329 333 334 335 340 342\n",
      " 346 351 352 357 360 367 377 380 381 388 393 395 412 417 429 432 439 440\n",
      " 443 446 449 453 464 465 477 478 481 485 497 506 514 518 525 529 532 535\n",
      " 538 539 547 554 557 559 570 572 583 586 589 593 594 599 608 615 616 618\n",
      " 619 626 631 632 637 640 643 666 671 673 675 679 682 685 703 705 707 713\n",
      " 718 719 735 745 746 749 755 758 762 766]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.0349 - loss: 0.3975 - val_accuracy: 0.2532 - val_loss: 0.0933\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5177 - loss: 0.0599 - val_accuracy: 0.7727 - val_loss: 0.0347\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.9179 - loss: 0.0188 - val_accuracy: 0.9610 - val_loss: 0.0160\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.9841 - loss: 0.0094 - val_accuracy: 0.9870 - val_loss: 0.0089\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9870 - val_loss: 0.0072\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9935 - val_loss: 0.0054\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.9935 - val_loss: 0.0051\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9935 - val_loss: 0.0046\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9935 - val_loss: 0.0043\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0040\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step\n",
      "Predicted Classes: [40 28  3 38 58 20 55 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  4 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      "  4 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 3\n",
      "Train Index [  1   2   3   4   5   6   7   8  10  11  13  14  15  16  18  20  21  23\n",
      "  24  26  27  28  29  30  31  32  33  34  35  36  37  39  40  41  42  43\n",
      "  44  45  47  48  49  50  51  52  53  54  55  56  58  60  61  62  63  64\n",
      "  65  66  67  69  70  71  72  73  74  76  77  78  79  80  81  82  83  84\n",
      "  85  86  87  89  90  91  92  95  96  97  98  99 101 102 103 104 105 106\n",
      " 108 109 110 111 112 114 118 119 120 121 122 123 127 128 129 130 131 132\n",
      " 133 134 135 136 137 138 139 140 143 145 146 147 148 150 151 152 155 156\n",
      " 157 158 159 160 161 162 163 164 165 166 167 168 170 171 174 175 176 177\n",
      " 178 181 182 183 186 187 189 191 192 193 194 196 197 198 199 200 201 202\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 223\n",
      " 224 225 226 227 229 230 231 232 233 234 235 237 239 240 241 242 243 244\n",
      " 246 247 248 250 251 252 253 254 256 257 258 259 260 262 263 264 265 266\n",
      " 267 269 270 273 275 276 279 281 282 283 285 286 288 290 291 292 293 294\n",
      " 295 296 297 299 300 302 303 306 308 309 311 313 314 315 316 317 319 323\n",
      " 324 325 326 327 328 329 330 331 333 334 335 336 337 338 339 340 342 343\n",
      " 344 345 346 347 348 350 351 352 355 356 357 358 360 361 362 363 364 366\n",
      " 367 370 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387\n",
      " 388 389 391 392 393 394 395 396 397 400 401 402 403 404 405 406 409 410\n",
      " 411 412 413 414 415 417 418 419 421 425 427 428 429 431 432 433 435 437\n",
      " 438 439 440 441 442 443 446 447 448 449 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 468 469 470 471 472 473 474 475 476 477\n",
      " 478 480 481 482 483 484 485 486 487 488 489 490 491 492 494 496 497 498\n",
      " 499 501 502 503 504 505 506 508 509 510 511 512 513 514 515 516 518 519\n",
      " 520 521 524 525 528 529 532 534 535 536 537 538 539 540 541 542 543 544\n",
      " 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 577 579 580 581 582 583 584\n",
      " 585 586 587 588 589 591 592 593 594 596 597 599 600 601 603 604 605 606\n",
      " 607 608 609 610 612 613 614 615 616 617 618 619 620 621 624 625 626 628\n",
      " 629 630 631 632 634 635 636 637 639 640 642 643 646 647 648 650 651 652\n",
      " 653 655 656 657 658 659 660 661 662 663 665 666 667 668 670 671 673 675\n",
      " 676 677 678 679 680 681 682 683 684 685 686 687 688 690 691 692 693 694\n",
      " 697 699 700 702 703 704 705 706 707 708 710 712 713 714 716 717 718 719\n",
      " 720 721 722 723 725 726 728 729 730 731 733 734 735 736 738 740 741 742\n",
      " 743 744 745 746 747 748 749 750 753 754 755 756 758 760 761 762 763 764\n",
      " 765 766]\n",
      "Test Index [  0   9  12  17  19  22  25  38  46  57  59  68  75  88  93  94 100 107\n",
      " 113 115 116 117 124 125 126 141 142 144 149 153 154 169 172 173 179 180\n",
      " 184 185 188 190 195 203 221 222 228 236 238 245 249 255 261 268 271 272\n",
      " 274 277 278 280 284 287 289 298 301 304 305 307 310 312 318 320 321 322\n",
      " 332 341 349 353 354 359 365 368 369 371 390 398 399 407 408 416 420 422\n",
      " 423 424 426 430 434 436 444 445 450 467 479 493 495 500 507 517 522 523\n",
      " 526 527 530 531 533 545 575 576 578 590 595 598 602 611 622 623 627 633\n",
      " 638 641 644 645 649 654 664 669 672 674 689 695 696 698 701 709 711 715\n",
      " 724 727 732 737 739 751 752 757 759 767]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.0370 - loss: 0.6331 - val_accuracy: 0.1688 - val_loss: 0.1108\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.3033 - loss: 0.0961 - val_accuracy: 0.5584 - val_loss: 0.0464\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.7245 - loss: 0.0401 - val_accuracy: 0.8831 - val_loss: 0.0289\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.9414 - loss: 0.0217 - val_accuracy: 0.9740 - val_loss: 0.0189\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.9904 - loss: 0.0139 - val_accuracy: 0.9935 - val_loss: 0.0144\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.9990 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0114\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.9988 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 0.0100\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0085\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0073\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0072\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step\n",
      "Predicted Classes: [40 28  3 38 58 20  4 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 30 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 4\n",
      "Train Index [  0   1   2   4   6   7   9  10  11  12  13  14  15  17  18  19  20  21\n",
      "  22  23  24  25  27  28  29  30  31  32  33  34  35  38  39  40  41  42\n",
      "  43  44  46  47  49  51  52  54  55  56  57  58  59  60  61  62  63  64\n",
      "  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  97  98  99 100 101\n",
      " 102 104 105 106 107 108 109 110 113 114 115 116 117 118 120 121 124 125\n",
      " 126 128 130 131 132 133 134 135 136 137 138 139 140 141 142 144 145 148\n",
      " 149 153 154 155 156 158 159 160 161 163 164 165 166 167 168 169 170 172\n",
      " 173 174 176 177 178 179 180 181 182 184 185 187 188 189 190 191 192 193\n",
      " 195 196 198 199 200 201 203 204 205 206 208 209 210 211 212 213 214 215\n",
      " 216 217 218 220 221 222 223 227 228 230 231 234 235 236 238 239 240 241\n",
      " 242 243 244 245 247 248 249 250 251 252 254 255 256 257 259 260 261 264\n",
      " 265 266 268 269 270 271 272 273 274 275 276 277 278 280 281 284 286 287\n",
      " 288 289 290 291 292 294 295 296 298 299 300 301 302 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 318 319 320 321 322 323 324 326 327 328 329\n",
      " 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 349\n",
      " 350 351 352 353 354 355 356 357 359 360 361 362 363 365 366 367 368 369\n",
      " 371 372 375 377 378 379 380 381 382 383 385 387 388 389 390 391 392 393\n",
      " 394 395 396 397 398 399 401 404 405 406 407 408 409 411 412 413 416 417\n",
      " 418 420 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 439\n",
      " 440 443 444 445 446 448 449 450 451 453 454 455 456 457 458 459 460 461\n",
      " 462 464 465 466 467 468 470 471 473 474 475 476 477 478 479 481 482 483\n",
      " 484 485 486 487 489 490 491 492 493 494 495 497 498 500 501 502 504 505\n",
      " 506 507 508 510 511 512 513 514 516 517 518 519 520 522 523 524 525 526\n",
      " 527 528 529 530 531 532 533 534 535 538 539 540 544 545 546 547 548 549\n",
      " 553 554 555 556 557 559 560 561 562 563 564 565 566 568 570 572 573 574\n",
      " 575 576 577 578 579 581 582 583 586 587 589 590 592 593 594 595 598 599\n",
      " 600 602 603 604 605 606 608 611 612 614 615 616 617 618 619 620 622 623\n",
      " 624 626 627 629 631 632 633 635 636 637 638 639 640 641 642 643 644 645\n",
      " 646 647 648 649 651 652 653 654 658 659 660 661 662 663 664 666 667 668\n",
      " 669 671 672 673 674 675 676 677 678 679 681 682 685 686 687 689 690 691\n",
      " 692 693 694 695 696 698 699 700 701 703 705 706 707 709 710 711 713 714\n",
      " 715 716 717 718 719 720 722 724 725 727 728 731 732 734 735 737 739 745\n",
      " 746 747 748 749 750 751 752 754 755 756 757 758 759 760 761 762 763 764\n",
      " 765 766 767]\n",
      "Test Index [  3   5   8  16  26  36  37  45  48  50  53  96 103 111 112 119 122 123\n",
      " 127 129 143 146 147 150 151 152 157 162 171 175 183 186 194 197 202 207\n",
      " 219 224 225 226 229 232 233 237 246 253 258 262 263 267 279 282 283 285\n",
      " 293 297 303 316 317 325 347 348 358 364 370 373 374 376 384 386 400 402\n",
      " 403 410 414 415 419 421 437 438 441 442 447 452 463 469 472 480 488 496\n",
      " 499 503 509 515 521 536 537 541 542 543 550 551 552 558 567 569 571 580\n",
      " 584 585 588 591 596 597 601 607 609 610 613 621 625 628 630 634 650 655\n",
      " 656 657 665 670 680 683 684 688 697 702 704 708 712 721 723 726 729 730\n",
      " 733 736 738 740 741 742 743 744 753]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 229ms/step - accuracy: 0.0527 - loss: 0.3116 - val_accuracy: 0.3399 - val_loss: 0.0834\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.5964 - loss: 0.0539 - val_accuracy: 0.7582 - val_loss: 0.0350\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 0.9092 - loss: 0.0187 - val_accuracy: 0.9739 - val_loss: 0.0115\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.9912 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9935 - val_loss: 0.0059\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9935 - val_loss: 0.0051\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.9935 - val_loss: 0.0045\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9935 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 128ms/step\n",
      "Predicted Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13 27 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 5\n",
      "Train Index [  0   2   3   5   6   7   8   9  10  11  12  15  16  17  18  19  22  23\n",
      "  24  25  26  28  29  30  31  33  36  37  38  39  41  42  43  44  45  46\n",
      "  48  49  50  51  53  54  55  56  57  59  60  61  63  65  66  67  68  69\n",
      "  70  72  73  74  75  76  77  78  79  81  82  83  84  86  88  89  90  92\n",
      "  93  94  96  97 100 101 103 104 107 108 109 110 111 112 113 114 115 116\n",
      " 117 118 119 120 122 123 124 125 126 127 129 131 132 133 135 136 137 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 157 158\n",
      " 162 163 164 165 167 168 169 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 185 186 188 190 192 193 194 195 196 197 198 199 202 203 204\n",
      " 207 208 209 210 211 212 213 215 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 231 232 233 234 235 236 237 238 239 244 245 246 247 248 249 250\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 271 272\n",
      " 274 275 277 278 279 280 281 282 283 284 285 286 287 289 290 291 292 293\n",
      " 294 296 297 298 299 300 301 302 303 304 305 306 307 309 310 311 312 314\n",
      " 316 317 318 319 320 321 322 323 324 325 326 327 328 329 331 332 333 334\n",
      " 335 336 338 340 341 342 344 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 367 368 369 370 371 373 374 375 376\n",
      " 377 380 381 382 383 384 386 388 390 393 394 395 396 398 399 400 402 403\n",
      " 404 405 407 408 409 410 411 412 414 415 416 417 419 420 421 422 423 424\n",
      " 425 426 428 429 430 431 432 433 434 436 437 438 439 440 441 442 443 444\n",
      " 445 446 447 448 449 450 451 452 453 456 457 462 463 464 465 467 468 469\n",
      " 470 472 473 477 478 479 480 481 482 483 485 486 487 488 490 493 494 495\n",
      " 496 497 499 500 501 503 506 507 509 511 512 513 514 515 516 517 518 519\n",
      " 521 522 523 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 541 542 543 544 545 547 548 549 550 551 552 554 557 558 559 567 568 569\n",
      " 570 571 572 575 576 578 579 580 581 582 583 584 585 586 587 588 589 590\n",
      " 591 593 594 595 596 597 598 599 601 602 603 604 605 606 607 608 609 610\n",
      " 611 613 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 643 644 645 648 649 650 654\n",
      " 655 656 657 659 662 664 665 666 667 668 669 670 671 672 673 674 675 677\n",
      " 679 680 682 683 684 685 688 689 690 691 692 694 695 696 697 698 701 702\n",
      " 703 704 705 706 707 708 709 711 712 713 714 715 716 718 719 720 721 723\n",
      " 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741\n",
      " 742 743 744 745 746 748 749 751 752 753 754 755 757 758 759 760 762 763\n",
      " 764 766 767]\n",
      "Test Index [  1   4  13  14  20  21  27  32  34  35  40  47  52  58  62  64  71  80\n",
      "  85  87  91  95  98  99 102 105 106 121 128 130 134 138 156 159 160 161\n",
      " 166 170 187 189 191 200 201 205 206 214 216 217 230 240 241 242 243 251\n",
      " 252 269 270 273 276 288 295 308 313 315 330 337 339 343 345 366 372 378\n",
      " 379 385 387 389 391 392 397 401 406 413 418 427 435 454 455 458 459 460\n",
      " 461 466 471 474 475 476 484 489 491 492 498 502 504 505 508 510 520 524\n",
      " 540 546 553 555 556 560 561 562 563 564 565 566 573 574 577 592 600 612\n",
      " 614 642 646 647 651 652 653 658 660 661 663 676 678 681 686 687 693 699\n",
      " 700 710 717 722 747 750 756 761 765]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.0391 - loss: 0.4138 - val_accuracy: 0.2549 - val_loss: 0.0950\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - accuracy: 0.4793 - loss: 0.0632 - val_accuracy: 0.7582 - val_loss: 0.0328\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.9068 - loss: 0.0173 - val_accuracy: 0.9673 - val_loss: 0.0149\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.9879 - loss: 0.0085 - val_accuracy: 0.9739 - val_loss: 0.0081\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9869 - val_loss: 0.0061\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.9869 - val_loss: 0.0057\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9869 - val_loss: 0.0043\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9869 - val_loss: 0.0043\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9869 - val_loss: 0.0039\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9869 - val_loss: 0.0038\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Final model saved to ../../models/data_with_augmentasi/training_batchSize30_epoch10.h5\n",
      "Accuracy for fold 1: 0.9791666666666666\n",
      "Accuracy for fold 2: 0.984375\n",
      "Accuracy for fold 3: 0.9895833333333334\n",
      "Accuracy for fold 4: 0.9947916666666666\n",
      "Accuracy for fold 5: 1.0\n",
      "Average Accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Load your image dataset and preprocessing here\n",
    "# Assume X is your image data and Y is your labels\n",
    "# Modify this part according to your image dataset loading and preprocessing\n",
    "\n",
    "# Define your image data (X) and labels (Y) here\n",
    "# Example: X = ... # Load images using some method (e.g., tf.keras.preprocessing.image_dataset_from_directory)\n",
    "X = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"../../dataset/dataset_restructure\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    image_size=(52, 52),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    ")\n",
    "# Example: Y = ... # Load labels (may need to be encoded)\n",
    "def extract_labels(image, label):\n",
    "    return label\n",
    "\n",
    "# Map the dataset to extract labels\n",
    "Y = X.map(extract_labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Convert dataset to numpy arrays\n",
    "X_list = []\n",
    "Y_list = []\n",
    "for images, labels in X.as_numpy_iterator():\n",
    "    X_list.extend(images)\n",
    "    Y_list.extend(labels)\n",
    "X = np.array(X_list)\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "Y_train = to_categorical(Y_train, num_classes=60)\n",
    "Y_test = to_categorical(Y_test, num_classes=60)\n",
    "\n",
    "# Define the ResNet50 model\n",
    "def build_resnet50_model():\n",
    "    # Membuat base model ResNet50 tanpa lapisan Fully Connected terakhir\n",
    "    base_model = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(52, 52, 3),\n",
    "    )\n",
    "\n",
    "    # Menghentikan layer-layer yang ada pada base model agar tidak ter-update selama pelatihan\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Membangun model dengan menambahkan lapisan kustom di atas base model\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    predictions = Dense(60, activation='softmax')(x)\n",
    "\n",
    "    # Membuat model gabungan\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize k-Fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Execute K-fold cross-validation\n",
    "fold_no = 1\n",
    "acc_per_fold = [] # Save accuracy from each fold\n",
    "\n",
    "# Train the model for each split (fold)\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print('Training for fold', fold_no)\n",
    "    print ('Train Index', train_index)\n",
    "    print ('Test Index', test_index)\n",
    "\n",
    "    # Get train and test data for this fold\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_val_fold = Y_train[train_index], Y_train[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = build_resnet50_model()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_fold, Y_train_fold, batch_size=30, epochs=10, validation_data=(X_val_fold, Y_val_fold))\n",
    "\n",
    "    # # Evaluate the model\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "    # accuracy = accuracy_score(Y_test, y_pred_binary)\n",
    "    # acc_per_fold.append(accuracy)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    true_classes = np.argmax(Y_test, axis=1)\n",
    "    accuracy = accuracy_score(true_classes, y_pred_classes)\n",
    "    acc_per_fold.append(accuracy)\n",
    "\n",
    "    # Print predicted and true classes\n",
    "    print(\"Predicted Classes:\", y_pred_classes)\n",
    "    print(\"True Classes:\", true_classes)\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "# Save the model after all folds are processed\n",
    "model_save_path = \"../../models/data_with_augmentasi/training_batchSize30_epoch10.h5\"\n",
    "model.save(model_save_path)\n",
    "print(f\"Final model saved to {model_save_path}\")\n",
    "\n",
    "# Print accuracy for each fold\n",
    "for i, acc in enumerate(acc_per_fold, 1):\n",
    "    print(f'Accuracy for fold {i}: {acc}')\n",
    "\n",
    "# Print Average Accuracy \n",
    "total_acc = sum(acc_per_fold)  # Summing up all accuracies\n",
    "num_folds = len(acc_per_fold)  # Getting the number of folds\n",
    "\n",
    "if num_folds > 0:\n",
    "    average_acc = total_acc / num_folds  # Calculating average accuracy\n",
    "    print(f'Average Accuracy: {average_acc:.3f}')\n",
    "else:\n",
    "    print(\"No folds to calculate average accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penambahan DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 960 files belonging to 60 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 17:03:37.375809: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1\n",
      "Train Index [  0   1   3   4   5   6   8   9  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  24  25  26  27  28  29  32  34  35  36  37  38  40  41  42  43\n",
      "  44  45  46  47  48  50  51  52  53  56  57  58  59  60  61  62  64  68\n",
      "  69  70  71  73  74  75  80  82  83  85  87  88  89  90  91  92  93  94\n",
      "  95  96  98  99 100 102 103 104 105 106 107 108 110 111 112 113 114 115\n",
      " 116 117 119 121 122 123 124 125 126 127 128 129 130 131 132 134 135 136\n",
      " 138 139 140 141 142 143 144 145 146 147 149 150 151 152 153 154 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 169 170 171 172 173 175 176 177\n",
      " 178 179 180 182 183 184 185 186 187 188 189 190 191 194 195 197 198 200\n",
      " 201 202 203 205 206 207 214 216 217 219 221 222 223 224 225 226 228 229\n",
      " 230 231 232 233 235 236 237 238 240 241 242 243 245 246 247 248 249 251\n",
      " 252 253 255 256 257 258 260 261 262 263 264 266 267 268 269 270 271 272\n",
      " 273 274 276 277 278 279 280 282 283 284 285 286 287 288 289 291 292 293\n",
      " 294 295 297 298 301 302 303 304 305 306 307 308 310 311 312 313 315 316\n",
      " 317 318 319 320 321 322 323 325 326 327 329 330 332 333 334 335 337 339\n",
      " 340 341 342 343 345 346 347 348 349 351 352 353 354 357 358 359 360 364\n",
      " 365 366 367 368 369 370 371 372 373 374 376 377 378 379 380 381 384 385\n",
      " 386 387 388 389 390 391 392 393 395 397 398 399 400 401 402 403 406 407\n",
      " 408 410 412 413 414 415 416 417 418 419 420 421 422 423 424 426 427 429\n",
      " 430 432 434 435 436 437 438 439 440 441 442 443 444 445 446 447 449 450\n",
      " 452 453 454 455 458 459 460 461 463 464 465 466 467 469 471 472 474 475\n",
      " 476 477 478 479 480 481 484 485 488 489 491 492 493 495 496 497 498 499\n",
      " 500 502 503 504 505 506 507 508 509 510 514 515 517 518 520 521 522 523\n",
      " 524 525 526 527 529 530 531 532 533 535 536 537 538 539 540 541 542 543\n",
      " 545 546 547 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564\n",
      " 565 566 567 569 570 571 572 573 574 575 576 577 578 580 583 584 585 586\n",
      " 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 607 608 609\n",
      " 610 611 612 613 614 615 616 618 619 621 622 623 625 626 627 628 630 631\n",
      " 632 633 634 637 638 640 641 642 643 644 645 646 647 649 650 651 652 653\n",
      " 654 655 656 657 658 660 661 663 664 665 666 669 670 671 672 673 674 675\n",
      " 676 678 679 680 681 682 683 684 685 686 687 688 689 693 695 696 697 698\n",
      " 699 700 701 702 703 704 705 707 708 709 710 711 712 713 715 717 718 719\n",
      " 721 722 723 724 726 727 729 730 732 733 735 736 737 738 739 740 741 742\n",
      " 743 744 745 746 747 749 750 751 752 753 755 756 757 758 759 761 762 765\n",
      " 766 767]\n",
      "Test Index [  2   7  10  23  30  31  33  39  49  54  55  63  65  66  67  72  76  77\n",
      "  78  79  81  84  86  97 101 109 118 120 133 137 148 155 168 174 181 192\n",
      " 193 196 199 204 208 209 210 211 212 213 215 218 220 227 234 239 244 250\n",
      " 254 259 265 275 281 290 296 299 300 309 314 324 328 331 336 338 344 350\n",
      " 355 356 361 362 363 375 382 383 394 396 404 405 409 411 425 428 431 433\n",
      " 448 451 456 457 462 468 470 473 482 483 486 487 490 494 501 511 512 513\n",
      " 516 519 528 534 544 548 549 568 579 581 582 587 603 604 605 606 617 620\n",
      " 624 629 635 636 639 648 659 662 667 668 677 690 691 692 694 706 714 716\n",
      " 720 725 728 731 734 748 754 760 763 764]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.0197 - loss: 0.8672 - val_accuracy: 0.0260 - val_loss: 0.1338\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.0475 - loss: 0.3114 - val_accuracy: 0.0844 - val_loss: 0.0922\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 0.0612 - loss: 0.1742 - val_accuracy: 0.3377 - val_loss: 0.0737\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.1200 - loss: 0.1302 - val_accuracy: 0.4545 - val_loss: 0.0670\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.1290 - loss: 0.1125 - val_accuracy: 0.3896 - val_loss: 0.0613\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 159ms/step - accuracy: 0.1574 - loss: 0.1005 - val_accuracy: 0.6688 - val_loss: 0.0557\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.2695 - loss: 0.0887 - val_accuracy: 0.6948 - val_loss: 0.0487\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - accuracy: 0.3352 - loss: 0.0818 - val_accuracy: 0.7662 - val_loss: 0.0547\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.3662 - loss: 0.0734 - val_accuracy: 0.8571 - val_loss: 0.0470\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.3924 - loss: 0.0695 - val_accuracy: 0.8506 - val_loss: 0.0419\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step\n",
      "Predicted Classes: [40 28  3 34 58 20 30 51 56 13 17 28 48 14 47 35 51 45 52 59 23 36 38 24\n",
      " 59 43 57 12 36  2 35 32 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 50 16 59  5 45 30 31 34 41 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 30 35  6 25  4 24 42 15 28  4\n",
      " 35 54 31 44 57  6  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 41 37 13 31 43 50 16  6  6 25 44 30  3\n",
      " 32 27 58  5 17 39 53 57 13 32 18 42 37  8 50 25 32 57 18 43 40 16 16 53\n",
      " 30 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33  8 54 37 23 44  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 2\n",
      "Train Index [  0   1   2   3   4   5   7   8   9  10  12  13  14  16  17  19  20  21\n",
      "  22  23  25  26  27  30  31  32  33  34  35  36  37  38  39  40  45  46\n",
      "  47  48  49  50  52  53  54  55  57  58  59  62  63  64  65  66  67  68\n",
      "  71  72  75  76  77  78  79  80  81  84  85  86  87  88  91  93  94  95\n",
      "  96  97  98  99 100 101 102 103 105 106 107 109 111 112 113 115 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 130 133 134 137 138 141\n",
      " 142 143 144 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161\n",
      " 162 166 168 169 170 171 172 173 174 175 179 180 181 183 184 185 186 187\n",
      " 188 189 190 191 192 193 194 195 196 197 199 200 201 202 203 204 205 206\n",
      " 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 224 225\n",
      " 226 227 228 229 230 232 233 234 236 237 238 239 240 241 242 243 244 245\n",
      " 246 249 250 251 252 253 254 255 258 259 261 262 263 265 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 287 288 289\n",
      " 290 293 295 296 297 298 299 300 301 303 304 305 307 308 309 310 312 313\n",
      " 314 315 316 317 318 320 321 322 324 325 328 330 331 332 336 337 338 339\n",
      " 341 343 344 345 347 348 349 350 353 354 355 356 358 359 361 362 363 364\n",
      " 365 366 368 369 370 371 372 373 374 375 376 378 379 382 383 384 385 386\n",
      " 387 389 390 391 392 394 396 397 398 399 400 401 402 403 404 405 406 407\n",
      " 408 409 410 411 413 414 415 416 418 419 420 421 422 423 424 425 426 427\n",
      " 428 430 431 433 434 435 436 437 438 441 442 444 445 447 448 450 451 452\n",
      " 454 455 456 457 458 459 460 461 462 463 466 467 468 469 470 471 472 473\n",
      " 474 475 476 479 480 482 483 484 486 487 488 489 490 491 492 493 494 495\n",
      " 496 498 499 500 501 502 503 504 505 507 508 509 510 511 512 513 515 516\n",
      " 517 519 520 521 522 523 524 526 527 528 530 531 533 534 536 537 540 541\n",
      " 542 543 544 545 546 548 549 550 551 552 553 555 556 558 560 561 562 563\n",
      " 564 565 566 567 568 569 571 573 574 575 576 577 578 579 580 581 582 584\n",
      " 585 587 588 590 591 592 595 596 597 598 600 601 602 603 604 605 606 607\n",
      " 609 610 611 612 613 614 617 620 621 622 623 624 625 627 628 629 630 633\n",
      " 634 635 636 638 639 641 642 644 645 646 647 648 649 650 651 652 653 654\n",
      " 655 656 657 658 659 660 661 662 663 664 665 667 668 669 670 672 674 676\n",
      " 677 678 680 681 683 684 686 687 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 701 702 704 706 708 709 710 711 712 714 715 716 717 720 721\n",
      " 722 723 724 725 726 727 728 729 730 731 732 733 734 736 737 738 739 740\n",
      " 741 742 743 744 747 748 750 751 752 753 754 756 757 759 760 761 763 764\n",
      " 765 767]\n",
      "Test Index [  6  11  15  18  24  28  29  41  42  43  44  51  56  60  61  69  70  73\n",
      "  74  82  83  89  90  92 104 108 110 114 131 132 135 136 139 140 145 158\n",
      " 163 164 165 167 176 177 178 182 198 223 231 235 247 248 256 257 260 264\n",
      " 266 286 291 292 294 302 306 311 319 323 326 327 329 333 334 335 340 342\n",
      " 346 351 352 357 360 367 377 380 381 388 393 395 412 417 429 432 439 440\n",
      " 443 446 449 453 464 465 477 478 481 485 497 506 514 518 525 529 532 535\n",
      " 538 539 547 554 557 559 570 572 583 586 589 593 594 599 608 615 616 618\n",
      " 619 626 631 632 637 640 643 666 671 673 675 679 682 685 703 705 707 713\n",
      " 718 719 735 745 746 749 755 758 762 766]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.0343 - loss: 0.8787 - val_accuracy: 0.0519 - val_loss: 0.2451\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.0177 - loss: 0.3884 - val_accuracy: 0.0584 - val_loss: 0.0992\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.0418 - loss: 0.2244 - val_accuracy: 0.1299 - val_loss: 0.0750\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.0706 - loss: 0.1568 - val_accuracy: 0.2922 - val_loss: 0.0654\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.0866 - loss: 0.1320 - val_accuracy: 0.4091 - val_loss: 0.0579\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.1195 - loss: 0.1129 - val_accuracy: 0.5260 - val_loss: 0.0537\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.1706 - loss: 0.1038 - val_accuracy: 0.5390 - val_loss: 0.0567\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.1991 - loss: 0.0967 - val_accuracy: 0.6948 - val_loss: 0.0547\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.2010 - loss: 0.0923 - val_accuracy: 0.7338 - val_loss: 0.0606\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - accuracy: 0.2001 - loss: 0.0907 - val_accuracy: 0.7662 - val_loss: 0.0496\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step\n",
      "Predicted Classes: [ 7 33  3  4 10 20  8  8 56 29 29  8 48 14  4 55  8 45 52  1 23 36  4 24\n",
      " 59 55 57 12 36  2 35 20 54 41 56 22 34 11 14 42 10 44 32 35 21 40 12 42\n",
      " 49  2 20  5 45  4 31 34 10 16 33 29 12 35 19 23 45 29 34 57 23 52 48  5\n",
      " 48 14  4 24  6 32 34 24 26  9 21 19 29  3  8 35 33 25  4 24 42 31  9  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 20  1 10 49 10 27  1  7 41 10 34 32 40\n",
      "  5 56 19  6 48 33 25 15 52 41  3 18 33  1 49 43  8 16  6  6 25 44 30  3\n",
      "  1 27 58 34 17 39 53 57  4 32  9 42 34  4 50 25 32 57  9 43 40 16  1 53\n",
      "  4 40  1 20 56 49 48  7 44 41 29 29 40 39 27 47 16 33 35 54 33 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 3\n",
      "Train Index [  1   2   3   4   5   6   7   8  10  11  13  14  15  16  18  20  21  23\n",
      "  24  26  27  28  29  30  31  32  33  34  35  36  37  39  40  41  42  43\n",
      "  44  45  47  48  49  50  51  52  53  54  55  56  58  60  61  62  63  64\n",
      "  65  66  67  69  70  71  72  73  74  76  77  78  79  80  81  82  83  84\n",
      "  85  86  87  89  90  91  92  95  96  97  98  99 101 102 103 104 105 106\n",
      " 108 109 110 111 112 114 118 119 120 121 122 123 127 128 129 130 131 132\n",
      " 133 134 135 136 137 138 139 140 143 145 146 147 148 150 151 152 155 156\n",
      " 157 158 159 160 161 162 163 164 165 166 167 168 170 171 174 175 176 177\n",
      " 178 181 182 183 186 187 189 191 192 193 194 196 197 198 199 200 201 202\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 223\n",
      " 224 225 226 227 229 230 231 232 233 234 235 237 239 240 241 242 243 244\n",
      " 246 247 248 250 251 252 253 254 256 257 258 259 260 262 263 264 265 266\n",
      " 267 269 270 273 275 276 279 281 282 283 285 286 288 290 291 292 293 294\n",
      " 295 296 297 299 300 302 303 306 308 309 311 313 314 315 316 317 319 323\n",
      " 324 325 326 327 328 329 330 331 333 334 335 336 337 338 339 340 342 343\n",
      " 344 345 346 347 348 350 351 352 355 356 357 358 360 361 362 363 364 366\n",
      " 367 370 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387\n",
      " 388 389 391 392 393 394 395 396 397 400 401 402 403 404 405 406 409 410\n",
      " 411 412 413 414 415 417 418 419 421 425 427 428 429 431 432 433 435 437\n",
      " 438 439 440 441 442 443 446 447 448 449 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 468 469 470 471 472 473 474 475 476 477\n",
      " 478 480 481 482 483 484 485 486 487 488 489 490 491 492 494 496 497 498\n",
      " 499 501 502 503 504 505 506 508 509 510 511 512 513 514 515 516 518 519\n",
      " 520 521 524 525 528 529 532 534 535 536 537 538 539 540 541 542 543 544\n",
      " 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 577 579 580 581 582 583 584\n",
      " 585 586 587 588 589 591 592 593 594 596 597 599 600 601 603 604 605 606\n",
      " 607 608 609 610 612 613 614 615 616 617 618 619 620 621 624 625 626 628\n",
      " 629 630 631 632 634 635 636 637 639 640 642 643 646 647 648 650 651 652\n",
      " 653 655 656 657 658 659 660 661 662 663 665 666 667 668 670 671 673 675\n",
      " 676 677 678 679 680 681 682 683 684 685 686 687 688 690 691 692 693 694\n",
      " 697 699 700 702 703 704 705 706 707 708 710 712 713 714 716 717 718 719\n",
      " 720 721 722 723 725 726 728 729 730 731 733 734 735 736 738 740 741 742\n",
      " 743 744 745 746 747 748 749 750 753 754 755 756 758 760 761 762 763 764\n",
      " 765 766]\n",
      "Test Index [  0   9  12  17  19  22  25  38  46  57  59  68  75  88  93  94 100 107\n",
      " 113 115 116 117 124 125 126 141 142 144 149 153 154 169 172 173 179 180\n",
      " 184 185 188 190 195 203 221 222 228 236 238 245 249 255 261 268 271 272\n",
      " 274 277 278 280 284 287 289 298 301 304 305 307 310 312 318 320 321 322\n",
      " 332 341 349 353 354 359 365 368 369 371 390 398 399 407 408 416 420 422\n",
      " 423 424 426 430 434 436 444 445 450 467 479 493 495 500 507 517 522 523\n",
      " 526 527 530 531 533 545 575 576 578 590 595 598 602 611 622 623 627 633\n",
      " 638 641 644 645 649 654 664 669 672 674 689 695 696 698 701 709 711 715\n",
      " 724 727 732 737 739 751 752 757 759 767]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.0420 - loss: 0.8581 - val_accuracy: 0.0195 - val_loss: 0.1397\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.0298 - loss: 0.2997 - val_accuracy: 0.0844 - val_loss: 0.0852\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.0580 - loss: 0.1972 - val_accuracy: 0.1753 - val_loss: 0.0757\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.0664 - loss: 0.1408 - val_accuracy: 0.3052 - val_loss: 0.0635\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.1483 - loss: 0.1139 - val_accuracy: 0.5779 - val_loss: 0.0521\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - accuracy: 0.2363 - loss: 0.0968 - val_accuracy: 0.6299 - val_loss: 0.0539\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.2760 - loss: 0.0874 - val_accuracy: 0.7987 - val_loss: 0.0469\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.2823 - loss: 0.0799 - val_accuracy: 0.8701 - val_loss: 0.0371\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.3893 - loss: 0.0715 - val_accuracy: 0.8831 - val_loss: 0.0418\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.3695 - loss: 0.0691 - val_accuracy: 0.9351 - val_loss: 0.0334\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step\n",
      "Predicted Classes: [40 28  3 21 58 20 18 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35 15 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46  4 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 28  0 44 57  3  9  2 32 36 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13 15 43 36 16  6  6 25 44 30  3\n",
      " 18 27 29  5 17 39 53 57 13 32 18 42 16 38 50 25 32 57 18 43 40 16 21 53\n",
      " 55 40 46 51 56 13 48  7 44 15 21 13 40 39 27 47 11 33 13 54 37 23 18  0]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 4\n",
      "Train Index [  0   1   2   4   6   7   9  10  11  12  13  14  15  17  18  19  20  21\n",
      "  22  23  24  25  27  28  29  30  31  32  33  34  35  38  39  40  41  42\n",
      "  43  44  46  47  49  51  52  54  55  56  57  58  59  60  61  62  63  64\n",
      "  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  97  98  99 100 101\n",
      " 102 104 105 106 107 108 109 110 113 114 115 116 117 118 120 121 124 125\n",
      " 126 128 130 131 132 133 134 135 136 137 138 139 140 141 142 144 145 148\n",
      " 149 153 154 155 156 158 159 160 161 163 164 165 166 167 168 169 170 172\n",
      " 173 174 176 177 178 179 180 181 182 184 185 187 188 189 190 191 192 193\n",
      " 195 196 198 199 200 201 203 204 205 206 208 209 210 211 212 213 214 215\n",
      " 216 217 218 220 221 222 223 227 228 230 231 234 235 236 238 239 240 241\n",
      " 242 243 244 245 247 248 249 250 251 252 254 255 256 257 259 260 261 264\n",
      " 265 266 268 269 270 271 272 273 274 275 276 277 278 280 281 284 286 287\n",
      " 288 289 290 291 292 294 295 296 298 299 300 301 302 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 318 319 320 321 322 323 324 326 327 328 329\n",
      " 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 349\n",
      " 350 351 352 353 354 355 356 357 359 360 361 362 363 365 366 367 368 369\n",
      " 371 372 375 377 378 379 380 381 382 383 385 387 388 389 390 391 392 393\n",
      " 394 395 396 397 398 399 401 404 405 406 407 408 409 411 412 413 416 417\n",
      " 418 420 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 439\n",
      " 440 443 444 445 446 448 449 450 451 453 454 455 456 457 458 459 460 461\n",
      " 462 464 465 466 467 468 470 471 473 474 475 476 477 478 479 481 482 483\n",
      " 484 485 486 487 489 490 491 492 493 494 495 497 498 500 501 502 504 505\n",
      " 506 507 508 510 511 512 513 514 516 517 518 519 520 522 523 524 525 526\n",
      " 527 528 529 530 531 532 533 534 535 538 539 540 544 545 546 547 548 549\n",
      " 553 554 555 556 557 559 560 561 562 563 564 565 566 568 570 572 573 574\n",
      " 575 576 577 578 579 581 582 583 586 587 589 590 592 593 594 595 598 599\n",
      " 600 602 603 604 605 606 608 611 612 614 615 616 617 618 619 620 622 623\n",
      " 624 626 627 629 631 632 633 635 636 637 638 639 640 641 642 643 644 645\n",
      " 646 647 648 649 651 652 653 654 658 659 660 661 662 663 664 666 667 668\n",
      " 669 671 672 673 674 675 676 677 678 679 681 682 685 686 687 689 690 691\n",
      " 692 693 694 695 696 698 699 700 701 703 705 706 707 709 710 711 713 714\n",
      " 715 716 717 718 719 720 722 724 725 727 728 731 732 734 735 737 739 745\n",
      " 746 747 748 749 750 751 752 754 755 756 757 758 759 760 761 762 763 764\n",
      " 765 766 767]\n",
      "Test Index [  3   5   8  16  26  36  37  45  48  50  53  96 103 111 112 119 122 123\n",
      " 127 129 143 146 147 150 151 152 157 162 171 175 183 186 194 197 202 207\n",
      " 219 224 225 226 229 232 233 237 246 253 258 262 263 267 279 282 283 285\n",
      " 293 297 303 316 317 325 347 348 358 364 370 373 374 376 384 386 400 402\n",
      " 403 410 414 415 419 421 437 438 441 442 447 452 463 469 472 480 488 496\n",
      " 499 503 509 515 521 536 537 541 542 543 550 551 552 558 567 569 571 580\n",
      " 584 585 588 591 596 597 601 607 609 610 613 621 625 628 630 634 650 655\n",
      " 656 657 665 670 680 683 684 688 697 702 704 708 712 721 723 726 729 730\n",
      " 733 736 738 740 741 742 743 744 753]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.0200 - loss: 0.9355 - val_accuracy: 0.0392 - val_loss: 0.2773\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.0240 - loss: 0.4053 - val_accuracy: 0.0523 - val_loss: 0.1130\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - accuracy: 0.0205 - loss: 0.2386 - val_accuracy: 0.1438 - val_loss: 0.0761\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.0266 - loss: 0.1767 - val_accuracy: 0.1961 - val_loss: 0.0703\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.0545 - loss: 0.1365 - val_accuracy: 0.3268 - val_loss: 0.0636\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.0985 - loss: 0.1238 - val_accuracy: 0.3529 - val_loss: 0.0636\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.1118 - loss: 0.1046 - val_accuracy: 0.4771 - val_loss: 0.0647\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.1532 - loss: 0.1069 - val_accuracy: 0.5359 - val_loss: 0.0562\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.1643 - loss: 0.0975 - val_accuracy: 0.6275 - val_loss: 0.0563\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.1859 - loss: 0.0918 - val_accuracy: 0.5817 - val_loss: 0.0573\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step\n",
      "Predicted Classes: [42 28  3 38 58 12 10 51 56 13 17 28 48 12 47 36 51 12 52 12 10 36  8 24\n",
      " 37 12 57 12 36  9 35  8 12 41 56 22 28 28 19 57 43 44 32  5 12 42 12 42\n",
      " 12 12 41  5 45 12 12 34 38 16 33 17 12 10 19 10 45 29 37 57 10 52 48 28\n",
      " 48 12 12 24 12 32 37 24 26  9 21 19 13 10 38 35 12 25 12 24 42 15 28 12\n",
      " 35 25 41 44 51 28  9  2 32 12 44 51 12 58 12 43 12  1  7 41 58 12 32 42\n",
      " 28 56 19 12 48 28 21 15 52 41 10 18 12 56 12 43 12 16 12 12 25 44 12 10\n",
      " 19 12 58  5 17 28  8 57 12 32 18 42 37 12 10 12 32 57 18 43 40 16 38  8\n",
      " 12 42 12 51 56 12 48  7 44  0 29 12 42 28 12 47 28 33 38 54 12 28 18 41]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Training for fold 5\n",
      "Train Index [  0   2   3   5   6   7   8   9  10  11  12  15  16  17  18  19  22  23\n",
      "  24  25  26  28  29  30  31  33  36  37  38  39  41  42  43  44  45  46\n",
      "  48  49  50  51  53  54  55  56  57  59  60  61  63  65  66  67  68  69\n",
      "  70  72  73  74  75  76  77  78  79  81  82  83  84  86  88  89  90  92\n",
      "  93  94  96  97 100 101 103 104 107 108 109 110 111 112 113 114 115 116\n",
      " 117 118 119 120 122 123 124 125 126 127 129 131 132 133 135 136 137 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 157 158\n",
      " 162 163 164 165 167 168 169 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 185 186 188 190 192 193 194 195 196 197 198 199 202 203 204\n",
      " 207 208 209 210 211 212 213 215 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 231 232 233 234 235 236 237 238 239 244 245 246 247 248 249 250\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 271 272\n",
      " 274 275 277 278 279 280 281 282 283 284 285 286 287 289 290 291 292 293\n",
      " 294 296 297 298 299 300 301 302 303 304 305 306 307 309 310 311 312 314\n",
      " 316 317 318 319 320 321 322 323 324 325 326 327 328 329 331 332 333 334\n",
      " 335 336 338 340 341 342 344 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 367 368 369 370 371 373 374 375 376\n",
      " 377 380 381 382 383 384 386 388 390 393 394 395 396 398 399 400 402 403\n",
      " 404 405 407 408 409 410 411 412 414 415 416 417 419 420 421 422 423 424\n",
      " 425 426 428 429 430 431 432 433 434 436 437 438 439 440 441 442 443 444\n",
      " 445 446 447 448 449 450 451 452 453 456 457 462 463 464 465 467 468 469\n",
      " 470 472 473 477 478 479 480 481 482 483 485 486 487 488 490 493 494 495\n",
      " 496 497 499 500 501 503 506 507 509 511 512 513 514 515 516 517 518 519\n",
      " 521 522 523 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 541 542 543 544 545 547 548 549 550 551 552 554 557 558 559 567 568 569\n",
      " 570 571 572 575 576 578 579 580 581 582 583 584 585 586 587 588 589 590\n",
      " 591 593 594 595 596 597 598 599 601 602 603 604 605 606 607 608 609 610\n",
      " 611 613 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 643 644 645 648 649 650 654\n",
      " 655 656 657 659 662 664 665 666 667 668 669 670 671 672 673 674 675 677\n",
      " 679 680 682 683 684 685 688 689 690 691 692 694 695 696 697 698 701 702\n",
      " 703 704 705 706 707 708 709 711 712 713 714 715 716 718 719 720 721 723\n",
      " 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741\n",
      " 742 743 744 745 746 748 749 751 752 753 754 755 757 758 759 760 762 763\n",
      " 764 766 767]\n",
      "Test Index [  1   4  13  14  20  21  27  32  34  35  40  47  52  58  62  64  71  80\n",
      "  85  87  91  95  98  99 102 105 106 121 128 130 134 138 156 159 160 161\n",
      " 166 170 187 189 191 200 201 205 206 214 216 217 230 240 241 242 243 251\n",
      " 252 269 270 273 276 288 295 308 313 315 330 337 339 343 345 366 372 378\n",
      " 379 385 387 389 391 392 397 401 406 413 418 427 435 454 455 458 459 460\n",
      " 461 466 471 474 475 476 484 489 491 492 498 502 504 505 508 510 520 524\n",
      " 540 546 553 555 556 560 561 562 563 564 565 566 573 574 577 592 600 612\n",
      " 614 642 646 647 651 652 653 658 660 661 663 676 678 681 686 687 693 699\n",
      " 700 710 717 722 747 750 756 761 765]\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.0128 - loss: 0.8711 - val_accuracy: 0.0654 - val_loss: 0.1622\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.0323 - loss: 0.3485 - val_accuracy: 0.1438 - val_loss: 0.0800\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - accuracy: 0.0451 - loss: 0.2014 - val_accuracy: 0.2026 - val_loss: 0.0693\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.0738 - loss: 0.1458 - val_accuracy: 0.4641 - val_loss: 0.0572\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.1240 - loss: 0.1204 - val_accuracy: 0.5359 - val_loss: 0.0623\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - accuracy: 0.1818 - loss: 0.1040 - val_accuracy: 0.6928 - val_loss: 0.0466\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.2205 - loss: 0.0965 - val_accuracy: 0.7712 - val_loss: 0.0536\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.2619 - loss: 0.0875 - val_accuracy: 0.8954 - val_loss: 0.0409\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.3233 - loss: 0.0756 - val_accuracy: 0.8758 - val_loss: 0.0355\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.3296 - loss: 0.0741 - val_accuracy: 0.8824 - val_loss: 0.0351\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [40 28  3 38 58 20 46 19 56 13 17 37 48 37 47 55 19 45 52 46 23 36  4 24\n",
      " 59 55 57 12 36  2 35 19 59 41 56 22  5 47 41 57 43 44 32  5 21 40 12 42\n",
      " 46  9 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48  6 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 47  4\n",
      " 35 25 41 44 57  3  9  2 32  6 44 19  1 58 46 43 27  1  7 41 58  6 32 40\n",
      "  5 56 19  6 48 47 21 15 52 41  3 18  6 13  1 43 46 16  6  6 25 44 30  3\n",
      " 37 27 58  5 17 47 53 57 13 32 18 42 37 38 50 25 32 57 50 43 40 16 38 53\n",
      "  8 40 46 51 56  1 48  7 44 29 21 13 40 47 27 47 11 33 38 54  6 23 18 41]\n",
      "True Classes: [40 28  3 38 58 20 46 51 56 13 17 28 48 14 47 55 51 45 52 46 23 36  1 24\n",
      " 59 55 57 12 36  2 35  0 59 41 56 22  5 11 14 57 43 44 32  5 21 40 12 42\n",
      " 46 46 59  5 45 55 31 34 38 16 33 17 12 35 19 23 45 29 37 57 23 52 48  5\n",
      " 48 14 38 24  6 32 37 24 26  9 21 19 13  3 38 35  6 25  4 24 42 15 28  4\n",
      " 35 25  0 44 57  3  9  2 32  1 44 51  1 58 46 43 27  1  7 41 58 14 32 40\n",
      "  5 56 19  6 48 28 21 15 52 41  3 18 37 13  0 43 46 16  6  6 25 44 30  3\n",
      "  0 27 58  5 17 39 53 57 13 32 18 42 37 38 50 25 32 57 18 43 40 16 38 53\n",
      " 55 40 46 51 56  1 48  7 44  0 21 13 40 39 27 47 11 33 38 54 37 23 18  0]\n",
      "Final model saved to ../../models/data_with_augmentasi/training_withDropOut_batchSize30_epoch10.h5\n",
      "Accuracy for fold 1: 0.875\n",
      "Accuracy for fold 2: 0.671875\n",
      "Accuracy for fold 3: 0.921875\n",
      "Accuracy for fold 4: 0.5989583333333334\n",
      "Accuracy for fold 5: 0.8645833333333334\n",
      "Average Accuracy: 0.786\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Load your image dataset and preprocessing here\n",
    "# Assume X is your image data and Y is your labels\n",
    "# Modify this part according to your image dataset loading and preprocessing\n",
    "\n",
    "# Define your image data (X) and labels (Y) here\n",
    "# Example: X = ... # Load images using some method (e.g., tf.keras.preprocessing.image_dataset_from_directory)\n",
    "X = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"../../dataset/dataset_restructure\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    image_size=(52, 52),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    ")\n",
    "# Example: Y = ... # Load labels (may need to be encoded)\n",
    "def extract_labels(image, label):\n",
    "    return label\n",
    "\n",
    "# Map the dataset to extract labels\n",
    "Y = X.map(extract_labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Convert dataset to numpy arrays\n",
    "X_list = []\n",
    "Y_list = []\n",
    "for images, labels in X.as_numpy_iterator():\n",
    "    X_list.extend(images)\n",
    "    Y_list.extend(labels)\n",
    "X = np.array(X_list)\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "Y_train = to_categorical(Y_train, num_classes=60)\n",
    "Y_test = to_categorical(Y_test, num_classes=60)\n",
    "\n",
    "# Define the ResNet50 model\n",
    "def build_resnet50_model():\n",
    "    # Membuat base model ResNet50 tanpa lapisan Fully Connected terakhir\n",
    "    base_model = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(52, 52, 3),\n",
    "    )\n",
    "\n",
    "    # Menghentikan layer-layer yang ada pada base model agar tidak ter-update selama pelatihan\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Membangun model dengan menambahkan lapisan kustom di atas base model\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)  # Adding dropout layer with 50% dropout rate\n",
    "    predictions = Dense(60, activation='softmax')(x)\n",
    "\n",
    "    # Membuat model gabungan\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize k-Fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Execute K-fold cross-validation\n",
    "fold_no = 1\n",
    "acc_per_fold = [] # Save accuracy from each fold\n",
    "\n",
    "# Train the model for each split (fold)\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print('Training for fold', fold_no)\n",
    "    print ('Train Index', train_index)\n",
    "    print ('Test Index', test_index)\n",
    "\n",
    "    # Get train and test data for this fold\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_val_fold = Y_train[train_index], Y_train[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = build_resnet50_model()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_fold, Y_train_fold, batch_size=30, epochs=10, validation_data=(X_val_fold, Y_val_fold))\n",
    "\n",
    "    # # Evaluate the model\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "    # accuracy = accuracy_score(Y_test, y_pred_binary)\n",
    "    # acc_per_fold.append(accuracy)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    true_classes = np.argmax(Y_test, axis=1)\n",
    "    accuracy = accuracy_score(true_classes, y_pred_classes)\n",
    "    acc_per_fold.append(accuracy)\n",
    "\n",
    "    # Print predicted and true classes\n",
    "    print(\"Predicted Classes:\", y_pred_classes)\n",
    "    print(\"True Classes:\", true_classes)\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "# Save the model after all folds are processed\n",
    "model_save_path = \"../../models/data_with_augmentasi/training_withDropOut_batchSize30_epoch10.h5\"\n",
    "model.save(model_save_path)\n",
    "print(f\"Final model saved to {model_save_path}\")\n",
    "\n",
    "# Print accuracy for each fold\n",
    "for i, acc in enumerate(acc_per_fold, 1):\n",
    "    print(f'Accuracy for fold {i}: {acc}')\n",
    "\n",
    "# Print Average Accuracy \n",
    "total_acc = sum(acc_per_fold)  # Summing up all accuracies\n",
    "num_folds = len(acc_per_fold)  # Getting the number of folds\n",
    "\n",
    "if num_folds > 0:\n",
    "    average_acc = total_acc / num_folds  # Calculating average accuracy\n",
    "    print(f'Average Accuracy: {average_acc:.3f}')\n",
    "else:\n",
    "    print(\"No folds to calculate average accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
